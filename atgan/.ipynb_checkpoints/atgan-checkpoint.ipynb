{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Age5-ZGqCtLY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-u7YeIHADpz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as K\n",
    "# import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"]= \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AiUKATSAx-l",
    "user_expressions": []
   },
   "source": [
    "### Saving and Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhNvZfWrAuMX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save(gan, generator, discriminator, model_folder, prefix=\"ACGAN\"):\n",
    "    \"\"\"\n",
    "    Save the model weights\n",
    "    Args:\n",
    "    - path: Path to save the model weights\n",
    "    - prefix: Prefix for the model weights\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    \n",
    "    #save generator and discriminator\n",
    "    discriminator.trainable = True\n",
    "    K.models.save_model(generator, f\"{model_folder}/{prefix}/generator\")\n",
    "    K.models.save_model(discriminator, f\"{model_folder}/{prefix}/discriminator\")\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    # save model\n",
    "    print(\"Saving AC-GAN\")\n",
    "    K.models.save_model(gan, f\"{model_folder}/{prefix}/model\")\n",
    "\n",
    "\n",
    "def load(model_folder, prefix=\"ACGAN\"):\n",
    "    \"\"\"\n",
    "    Load the model weights\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    generator = K.models.load_model(f\"{model_folder}/{prefix}/generator\")\n",
    "    discriminator = K.models.load_model(f\"{model_folder}/{prefix}/discriminator\")\n",
    "\n",
    "\n",
    "    gan = K.models.load_model(f\"{model_folder}/{prefix}/model\")\n",
    "    \n",
    "    generator.summary()\n",
    "    discriminator.summary()\n",
    "    \n",
    "    gan.summary()\n",
    "\n",
    "    return generator, discriminator, gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEpZGARzA1bs",
    "user_expressions": []
   },
   "source": [
    "# AC-GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7qeHpDMA9TS",
    "user_expressions": []
   },
   "source": [
    "## AC-GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "to3PUI0oA5mY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(K.Model):\n",
    "    \"\"\"\n",
    "    Generator component of AC-GAN for MNIST dataset\n",
    "\n",
    "    Args:\n",
    "    - latent_dim: Dimension of the latent space (generated as noise)\n",
    "    - n_classes: Number of classes(labels) in the dataset (default=10)\n",
    "\n",
    "    inherited from https://github.com/kochlisGit/Generative-Adversarial-Networks/blob/main/mnist-digits-acgan/digits-acgan.py\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_classes=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Layers for Latent Inputs\n",
    "        self.dense1 = K.layers.Dense(units=7 * 7 * 256, use_bias=False)\n",
    "        self.bn1 = K.layers.BatchNormalization()\n",
    "        self.reshape1 = K.layers.Reshape(target_shape=[7, 7, 256])\n",
    "\n",
    "        # Layers for Label Inputs\n",
    "        self.embedding = K.layers.Embedding(input_dim=n_classes, output_dim=64)\n",
    "        self.dense2 = K.layers.Dense(units=7*7, use_bias=False)\n",
    "        self.bn2 = K.layers.BatchNormalization()\n",
    "        self.reshape2 = K.layers.Reshape(target_shape=(7, 7, 1))\n",
    "\n",
    "        # Layers for Merging Inputs (Combining Latent and Label Inputs)\n",
    "        self.conv1 = K.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding='same', use_bias=False)\n",
    "        self.bn3 = K.layers.BatchNormalization()\n",
    "        self.dropout1 = K.layers.Dropout(rate=0.4)\n",
    "        self.conv2 = K.layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
    "        self.bn4 = K.layers.BatchNormalization()\n",
    "        self.dropout2 = K.layers.Dropout(rate=0.4)\n",
    "        self.conv3 = K.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of the generator\n",
    "        - latent_inputs: Random noise from the latent space, using for generating images\n",
    "        - label_inputs: Labels for the images to be generated\n",
    "        - training: Boolean flag for whether training or testing\n",
    "        \"\"\"\n",
    "        latent_inputs, label_inputs = inputs\n",
    "\n",
    "        # Latent Inputs Layer (Dense Layer + BatchNorm + ReLU + Reshape)\n",
    "        x1 = self.dense1(latent_inputs)\n",
    "        x1 = self.bn1(x1, training=training)\n",
    "        x1 = K.layers.LeakyReLU()(x1)\n",
    "        x1 = self.reshape1(x1)\n",
    "\n",
    "        # Process label inputs\n",
    "        x2 = self.embedding(label_inputs)\n",
    "        x2 = self.dense2(x2)\n",
    "        x2 = self.bn2(x2, training=training)\n",
    "        x2 = K.layers.LeakyReLU()(x2)\n",
    "        x2 = self.reshape2(x2)\n",
    "\n",
    "        #\n",
    "        merged_inputs = K.layers.Concatenate()([x1, x2])\n",
    "        x = self.conv1(merged_inputs)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = K.layers.LeakyReLU()(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = K.layers.LeakyReLU()(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsR-WKBJBABU",
    "user_expressions": []
   },
   "source": [
    "## AC-GAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3yrqKrnBCY2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(K.Model):\n",
    "    \"\"\"\n",
    "    Discriminator component of AC-GAN for MNIST dataset\n",
    "\n",
    "    Args:\n",
    "    - n_classes: Number of classes(labels) in the dataset (default=10) which predicted (discriminated) by the Discriminator\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Define layers\n",
    "        self.gaussian_noise = K.layers.GaussianNoise(stddev=0.2)\n",
    "        self.conv1 = K.layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
    "        self.bn1 = K.layers.BatchNormalization()\n",
    "        self.dropout1 = K.layers.Dropout(rate=0.4)\n",
    "        self.conv2 = K.layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
    "        self.bn2 = K.layers.BatchNormalization()\n",
    "        self.dropout2 = K.layers.Dropout(rate=0.4)\n",
    "\n",
    "        # flatten layer\n",
    "        self.flatten = K.layers.Flatten()\n",
    "\n",
    "        # Output layers: 2 Dense Layer for validity and label prediction\n",
    "        self.dense1 = K.layers.Dense(units=1, activation='sigmoid') # dense layer for validity the image\n",
    "        self.dense2 = K.layers.Dense(units=n_classes, activation='softmax') # dense layer for classifying the label\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of the discriminator\n",
    "        Args:\n",
    "        - inputs: Input images to be discriminated. Passing the input (generated by the Generator) through the Discriminator\n",
    "        and output the validity and label prediction\n",
    "        - training: Boolean flag for whether training or testing\n",
    "\n",
    "        Returns:\n",
    "        - validity: Validity of the input image that the discriminator predicts\n",
    "        - label: Label of the input image that the discriminator predicts\n",
    "        \"\"\"\n",
    "        x = self.gaussian_noise(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = K.layers.LeakyReLU()(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = K.layers.LeakyReLU()(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Output layers\n",
    "        validity = self.dense1(x)\n",
    "        label = self.dense2(x)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHK84CyQBUxa",
    "user_expressions": []
   },
   "source": [
    "## AC-GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De4TvXlVBZAi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ACGAN(K.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim, n_classes=10):\n",
    "        super(ACGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.generator_optimizer = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "        self.discriminator_optimizer = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "        # Define loss functions with label smoothing\n",
    "        self.binary_loss = K.losses.BinaryCrossentropy(label_smoothing=0.25) #\n",
    "        self.sparse_categorical_loss = K.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    def compile(self):\n",
    "        super(ACGAN, self).compile()\n",
    "\n",
    "        # Set the discriminator to not trainable initially\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # # Compile the combined model\n",
    "        # self.compile(\n",
    "        #     optimizer=self.generator_optimizer,\n",
    "        #     loss=[self.binary_loss, self.sparse_categorical_loss]\n",
    "        # )\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the ACGAN model.\n",
    "\n",
    "        Args:\n",
    "        - inputs: A list containing [latent_inputs, label_inputs] which refers to the random noise\n",
    "        - training: Boolean flag for whether training or testing\n",
    "\n",
    "        Returns:\n",
    "        - discriminated_validity: Validity of the input image that the discriminator predicts\n",
    "        - discriminated_label: Label of the input image that the discriminator predicts\n",
    "        \"\"\"\n",
    "        latent_inputs, label_inputs = inputs\n",
    "        generated_images = self.generator([latent_inputs, label_inputs], training=training)\n",
    "        discriminated_validity, discriminated_label = self.discriminator(generated_images, training=training)\n",
    "        return discriminated_validity, discriminated_label\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Training step for the ACGAN model\n",
    "        Args:\n",
    "        - data: A batch of real images getting from the dataset (i.e. MNIST), this contains the images and labels,\n",
    "        and the corresponding shape and size of the images\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = data\n",
    "        batch_size = tf.shape(x_batch)[0]\n",
    "\n",
    "        # =========================== Ground Truth labels =======================================\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "        mixed_labels = tf.concat([real_labels, fake_labels], axis=0)\n",
    "        #========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "        # ====================== Generate the Noise for Discriminator ===========================\n",
    "\n",
    "        # Generate random noise and random labels from the latent space\n",
    "        random_latent_noise = tf.random.normal(shape=[batch_size, self.latent_dim]) #shape=[32,128]\n",
    "        # Categorical labels, TODO: Using uint8 because we do not use One-Hot for Y-label\n",
    "        # random_labels = tf.random.uniform(shape=[batch_size], minval=0, maxval=self.n_classes, dtype=tf.float32) #shape=[32,?].\n",
    "        random_labels = np.random.randint(0, n_classes, size=[batch_size])\n",
    "        # random_labels_one_hot = tf.one_hot(random_labels, depth=self.n_classes, dtype=tf.uint8)\n",
    "\n",
    "        # Generate images from random noise and labels by Generator\n",
    "        generated_images = self.generator([random_latent_noise, random_labels], training=True)\n",
    "\n",
    "        # Mixed the real and generated images and labels for Discriminator (Concatenating)\n",
    "        mixed_images = tf.concat([x_batch, generated_images], axis=0)\n",
    "        mixed_generated_labels = tf.concat([y_batch, random_labels], axis=0)\n",
    "        # mixed_generated_labels = tf.concat([y_batch, random_labels_one_hot], axis=0)\n",
    "\n",
    "        #========================================================================================\n",
    "\n",
    "\n",
    "        # =========================== Train the Discriminator ====================================\n",
    "        self.discriminator.trainable = True # Set the discriminator to trainable\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            discriminated_validity, discriminated_label = self.discriminator(mixed_images, training=True)\n",
    "\n",
    "            discriminator_loss = [\n",
    "                self.binary_loss(mixed_labels, discriminated_validity), # validity loss\n",
    "                self.sparse_categorical_loss(mixed_generated_labels, discriminated_label) #label loss\n",
    "            ]\n",
    "\n",
    "            total_discriminator_loss = tf.reduce_mean(discriminator_loss[0]) + tf.reduce_mean(discriminator_loss[1])\n",
    "\n",
    "        gradients_D = tape.gradient(total_discriminator_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_D, self.discriminator.trainable_variables))\n",
    "\n",
    "        #========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "        # =========================== Train the Generator =================================================\n",
    "        self.discriminator.trainable = False # Set the discriminator to not trainable\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator([random_latent_noise, random_labels], training=True)\n",
    "            discriminated_validity, discriminated_label = self.discriminator(generated_images, training=False)\n",
    "\n",
    "            generator_loss = [\n",
    "                self.binary_loss(real_labels, discriminated_validity),\n",
    "                self.sparse_categorical_loss(random_labels, discriminated_label)\n",
    "            ]\n",
    "\n",
    "            total_generator_loss = tf.reduce_mean(generator_loss[0]) + tf.reduce_mean(generator_loss[1])\n",
    "\n",
    "        gradients_G = tape.gradient(total_generator_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_G, self.generator.trainable_variables))\n",
    "\n",
    "        #========================================================================================\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": total_discriminator_loss,\n",
    "            \"g_loss\": total_generator_loss\n",
    "        }\n",
    "\n",
    "\n",
    "    def generate_images(self, latent_space, labels):\n",
    "        \"\"\"\n",
    "        Generate images from the latent space and labels. Using Generator only.\n",
    "        Args:\n",
    "        - latent_space: Random noise from the latent space\n",
    "        - labels: Labels for the images to be generated\n",
    "        \"\"\"\n",
    "        return self.generator([latent_space, labels], training=False)\n",
    "\n",
    "\n",
    "# ================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39ukblvkBm1H",
    "user_expressions": []
   },
   "source": [
    "## Training AC-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiOLn-YKCNNU",
    "user_expressions": []
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-sHChHcCRRH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# mnist_train, mnist_test = tfds.load('mnist', split=['train', 'test'],data_dir='~/tensorflow_datasets',  as_supervised=True)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = K.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "print(\"Training set:\", x_train.shape)\n",
    "print(\"Training label:\", y_train.shape)\n",
    "print(\"Test set:\", x_test.shape)\n",
    "print(\"Test label:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Normalizing and Reshaping the data...\")\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype(np.float32)\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype(np.float32)\n",
    "x_text = (x_test - 127.5) / 127.5\n",
    "\n",
    "# NOT USED: Convert labels to one-hot encoding ---------------------------\n",
    "# BECAUSE IT MIXED WITH SHAPE OF RANDOM NEIGHBOR, WHICH IS [32] \n",
    "# USE ONE HOT LABEL MAKE SHAPE BECOME: [32,10]\n",
    "# y_train_one_hot = K.utils.to_categorical(y_train, num_classes=10)\n",
    "# y_test_one_hot = K.utils.to_categorical(y_test, num_classes=10)\n",
    "# y_train_one_hot = tf.cast(y_train_one_hot, dtype=tf.uint8)\n",
    "# y_test_one_hot = tf.cast(y_test_one_hot, dtype=tf.uint8) #cast to uint8\n",
    "# print(\"Train label one hot\", y_train_one_hot.shape)\n",
    "# print(\"Test label one hot\", y_test_one_hot.shape)\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "print(\"Train label\", y_train.shape)\n",
    "print(\"Test label\", y_test.shape)\n",
    "\n",
    "print(\"Completed preprocessing the data!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQmcx5XSCWX-",
    "user_expressions": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExZoBeZoCZuz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "latent_dim = 128 #noise size\n",
    "n_classes = 10\n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx7ZEUDcCb35",
    "user_expressions": []
   },
   "source": [
    "### Create Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DtGmFE_CDdC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the AC-GAN model\n",
    "generator = Generator(latent_dim, n_classes)\n",
    "discriminator = Discriminator(n_classes)\n",
    "\n",
    "acgan = ACGAN(generator, discriminator, latent_dim, n_classes)\n",
    "\n",
    "acgan.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYGDT_GwCkGL",
    "user_expressions": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAk5c_F2CmG3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# # Create the dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=x_train.shape[0])\n",
    "inputs = train_dataset.batch(batch_size=batch_size, drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "batches_per_epoch = x_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(inputs):\n",
    "        losses = acgan.train_step([x_batch, y_batch])\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Batch {i}/{batches_per_epoch}, Discriminator Loss: {losses['d_loss']}, Generator Loss: {losses['g_loss']}\")\n",
    "\n",
    "    print(f\"\\nEpoch ({epoch+1}/{epochs}): \\n Discriminator Loss: {losses['d_loss']}, Generator Loss: {losses['g_loss']}\\n\")\n",
    "\n",
    "print(\"Training complete!\")        \n",
    "# Save the model\n",
    "print(\"Saving the model\")\n",
    "model_folder = \"models\"\n",
    "save(acgan, generator, discriminator, model_folder, prefix=\"ACGAN\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_acgan(acgan, x_test, y_test, batch_size=32):\n",
    "#     \"\"\"Evaluates the AC-GAN using the discriminator's auxiliary classifier.\"\"\"\n",
    "#     _, aux_output = acgan.discriminator.predict(x_test, batch_size=batch_size)\n",
    "#     predicted_labels = np.argmax(aux_output, axis=1)\n",
    "#     accuracy = np.mean(predicted_labels == y_test)\n",
    "#     # print(f\"AC-GAN Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "#     return accuracy\n",
    "\n",
    "# accuracy = evaluate_acgan(acgan, x_test, y_test)\n",
    "# print(f\"AC-GAN Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits_per_class = 3\n",
    "# random_noise = tf.random.normal(shape=[digits_per_class * n_classes, latent_dim])\n",
    "# digit_targets = np.array([target for target in range(n_classes) for _ in range(digits_per_class)])\n",
    "# generated_digits = generator.predict([random_noise, digit_targets])\n",
    "\n",
    "# rows = 5\n",
    "# cols = 6\n",
    "# fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 10))\n",
    "# for i, digit in enumerate(generated_digits):\n",
    "#     digit = np.reshape(digit * 127.5 + 127.5, (28, 28))\n",
    "#     ax = axes[i // cols, i % cols]\n",
    "#     ax.imshow(digit, cmap='gray')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BDJneLRBiqD"
   },
   "source": [
    "# AT - GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsKp9cyBt99"
   },
   "source": [
    "## Target Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NceTp0bIBmUF"
   },
   "outputs": [],
   "source": [
    "# AT-GAN MODELS: Extended from ACGAN for Adversarial Attack\n",
    "class TargetClassifier(K.Model):\n",
    "    \"\"\"\n",
    "    Target Classifier for the AT-GAN model.\n",
    "    This simply acts as the classifier for the input images (MNIST) of either real or generated images.\n",
    "    Using as the target for the attack.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(TargetClassifier, self).__init__()\n",
    "\n",
    "        # Classifier Layers\n",
    "        self.conv1 = K.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1))\n",
    "        self.pool1 = K.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = K.layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n",
    "        self.pool2 = K.layers.MaxPooling2D((2, 2))\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwbPAPUdBr9I"
   },
   "source": [
    "## Attack Generator (`G_attack`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZP8IkqJB2aS"
   },
   "outputs": [],
   "source": [
    "class Attack_Generator(K.Model):\n",
    "    \"\"\"\n",
    "    G_attack simply a copy of AC-GAN Generator, and used for the adversarial attack.\n",
    "    Which transfering the output of the Generator to the Target Classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, generator):\n",
    "        super(Attack_Generator, self).__init__()\n",
    "        self.generator = generator\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.generator(inputs, training=training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NxWKxJyB5GA"
   },
   "source": [
    "## AT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJp7VSTxB7iD"
   },
   "outputs": [],
   "source": [
    "class ATGAN:\n",
    "    def __init__(self, G_original, G_attack, f_target, noise_size, lambda_adv_at=2.0, lambda_dist=1.0):\n",
    "        self.G_original = G_original # Original Generator (G_original)\n",
    "        self.G_attack = G_attack # Adversarial Generator (G_attack)\n",
    "        self.f_target = f_target    # Target Classifier (f_target)\n",
    "\n",
    "        self.noise_size = noise_size # latent space size\n",
    "\n",
    "        self.lambda_adv_at = lambda_adv_at  # lambda for adversarial loss\n",
    "        self.lambda_dist = lambda_dist     # lambda for distance loss\n",
    "\n",
    "        self.optimizer_G_attack = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        self.sparse_categorical_loss = K.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_atgan(self, images, target_labels):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "\n",
    "        with tf.GradientTape() as g_attack_tape:\n",
    "            z = tf.random.normal([batch_size, self.noise_size])\n",
    "\n",
    "            # Generate adversarial images\n",
    "            adv_images = self.G_attack([z, target_labels], training=True)\n",
    "\n",
    "            # Target classifier's prediction on adversarial images\n",
    "            pred_adv = self.f_target(adv_images, training=False)\n",
    "\n",
    "            # 1. Adversarial Loss (La) ========================================================\n",
    "\n",
    "            la_loss = tf.reduce_mean(\n",
    "                self.sparse_categorical_loss(target_labels, pred_adv)\n",
    "            )\n",
    "\n",
    "            # 2. Distance Loss (Ld) ========================================================\n",
    "            # Add Gaussian noise\n",
    "            noise = tf.random.normal(shape=tf.shape(adv_images), mean=0.0, stddev=0.1)\n",
    "            adv_images_noisy = adv_images + noise\n",
    "\n",
    "            # Original images generated by G_original\n",
    "            orig_images = self.G_original([z, target_labels], training=False)\n",
    "\n",
    "            ld_loss = tf.reduce_mean(tf.square(orig_images - adv_images_noisy))\n",
    "\n",
    "            # Total adversarial loss for G_attack\n",
    "            g_attack_loss = self.lambda_adv_at * la_loss + self.lambda_dist * ld_loss\n",
    "\n",
    "        # Calculate G_attack gradients\n",
    "        g_attack_gradients = g_attack_tape.gradient(g_attack_loss, self.G_attack.trainable_variables)\n",
    "        self.optimizer_G_attack.apply_gradients(zip(g_attack_gradients, self.G_attack.trainable_variables))\n",
    "\n",
    "        return g_attack_loss, la_loss, ld_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train AT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs_atgan = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train target classifier\n",
    "f_target = TargetClassifier()\n",
    "f_target.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "f_target.fit(x_train, y_train_one_hot, epochs=5, batch_size=batch_size, validation_data=(x_test, y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create G_attack and AT-GAN\n",
    "G_attack_instance = Attack_Generator(generator)\n",
    "atgan = ATGAN(generator, G_attack_instance, f_target, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AT-GAN\n",
    "for epoch in range(epochs_atgan):\n",
    "    print('\\nTraining AT-GAN on epoch', epoch + 1)\n",
    "    for i, (x_batch, _) in enumerate(inputs):\n",
    "        target_labels = np.random.randint(0, n_classes, size=[batch_size])\n",
    "        g_attack_loss, la_loss, ld_loss = atgan.train_step_atgan(x_batch, target_labels)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f'Batch {i}, G_attack Loss: {g_attack_loss}, La Loss: {la_loss}, Ld Loss: {ld_loss}')\n",
    "\n",
    "    print(f'\\nEpoch ({epoch + 1}/{epochs_atgan}):\\n G_attack Loss: {g_attack_loss}, La Loss: {la_loss}, Ld Loss: {ld_loss}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate AT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_atgan(atgan, f_target, x_test, y_test, noise_size=128, n_classes=10, num_batches=100):\n",
    "    \"\"\"\n",
    "    Evaluates the AT-GAN by generating adversarial examples and testing the target classifier.\n",
    "\n",
    "    Args:\n",
    "        atgan: The trained ATGAN model.\n",
    "        f_target: The target classifier to be attacked.\n",
    "        x_test: Test dataset images.\n",
    "        y_test: True labels for the test dataset.\n",
    "        noise_size: The dimension of the random noise vector.\n",
    "        n_classes: Number of classes in the dataset.\n",
    "        num_batches: Number of batches to use for evaluation.\n",
    "    \"\"\"\n",
    "    target_classifier_fooled = 0\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        batch_indices = np.random.choice(len(x_test), size=batch_size)\n",
    "        x_batch = x_test[batch_indices]\n",
    "        y_batch = y_test[batch_indices]\n",
    "\n",
    "        z = tf.random.normal([batch_size, noise_size])\n",
    "        \n",
    "        # Generate target labels that are different from the true labels\n",
    "        target_labels = (y_batch + np.random.randint(1, n_classes, size=batch_size)) % n_classes\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        adv_examples = atgan.G_attack([z, target_labels], training=False)\n",
    "\n",
    "        # Classify adversarial examples with the target classifier\n",
    "        predictions = f_target.predict(adv_examples)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Count how many times the target classifier was fooled\n",
    "        target_classifier_fooled += np.sum(predicted_labels == target_labels)\n",
    "\n",
    "    # Calculate the success rate of the attack\n",
    "    fooling_rate = (target_classifier_fooled / (num_batches * batch_size)) * 100\n",
    "    print(f\"AT-GAN Attack Success Rate: {fooling_rate:.2f}%\")\n",
    "    return fooling_rate\n",
    "\n",
    "attack_success_rate = evaluate_atgan(atgan, f_target, x_test, y_test)\n",
    "print(f\"AT-GAN Attack Success Rate: {attack_success_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_adv_examples(atgan, f_target, noise_size, n_classes, num_examples_per_class, save_dir):\n",
    "    \"\"\"Generates and saves adversarial examples using the AT-GAN.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for target_class in range(n_classes):\n",
    "        z = tf.random.normal([num_examples_per_class, noise_size])\n",
    "        target_labels = np.full((num_examples_per_class,), target_class)\n",
    "\n",
    "        adv_examples = atgan.G_attack([z, target_labels], training=False)\n",
    "        adv_examples = ((adv_examples + 1) * 127.5).numpy().astype(np.uint8)  # Rescale to 0-255\n",
    "\n",
    "        for i, adv_example in enumerate(adv_examples):\n",
    "            img = K.preprocessing.image.array_to_img(adv_example.reshape(28, 28, 1))\n",
    "            img.save(os.path.join(save_dir, f\"x_adv_{target_class}_{i}.png\"))\n",
    "\n",
    "# Example usage: Generate and save adversarial examples\n",
    "num_examples_per_class = 10\n",
    "save_dir = \"adversarial_examples\"\n",
    "generate_and_save_adv_examples(atgan, f_target, latent_dim, n_classes, num_examples_per_class, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B7qeHpDMA9TS",
    "VGsKp9cyBt99",
    "lwbPAPUdBr9I",
    "3NxWKxJyB5GA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
