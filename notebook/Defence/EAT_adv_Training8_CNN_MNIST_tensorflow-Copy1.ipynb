{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCL6UCvWmggf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Nov 22 2023, 10:22:35) \n",
      "[GCC 9.4.0]\n",
      "executable: \n",
      "/usr/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 14:54:24.262643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 14:54:24.375019: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-29 14:54:25.131810: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-29 14:54:25.131891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-29 14:54:25.131895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "4.9.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(\"executable: \")\n",
    "print(sys.executable)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_datasets as tfds\n",
    "print(tfds.__version__)\n",
    "#print(\"TFDS version:\", tfds.__version__)\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 14:54:31.102220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12978 MB memory:  -> device: 0, name: NVIDIA A16, pci bus id: 0000:1c:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "atn1 = tf.keras.models.load_model('ATN_base_model_2NDHALF.keras')\n",
    "atn2 = tf.keras.models.load_model('ATN_simpleCNN.keras')\n",
    "atn3 = tf.keras.models.load_model('ATN_StrongCNN.keras')\n",
    "atn1.trainable = False  # Freeze weights\n",
    "atn2.trainable = False  # Freeze weights\n",
    "atn3.trainable = False  # Freeze weights\n",
    "atns = [atn1, atn2, atn3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "_dUJNbrZmggi",
    "outputId": "3802d63c-a895-4db8-ab3f-6d73da0341ea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "#loading mnist data from tensorflow\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "#mnist = tfds.load(name='mnist', split='train', as_supervised=True)\n",
    "# Returns both train and test split separately\n",
    "mnist_train, mnist_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "def preprocess_image(image, label):\n",
    "    image = image\n",
    "    #image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1] # i believe this is not in the original code\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode the label\n",
    "    return image, label\n",
    "mnist_train = mnist_train.map(preprocess_image)\n",
    "mnist_test = mnist_test.map(preprocess_image)\n",
    "num_train_samples = len(list(mnist_train))\n",
    "mnist_train = mnist_train.take(num_train_samples // 2) #only first half of mnist train\n",
    "print(len(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKt0nwHfmggn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining the network\n",
    "input_width = 28\n",
    "input_height = 28\n",
    "input_channels = 1\n",
    "input_pixels = 784\n",
    "\n",
    "n_conv1 = 32\n",
    "n_conv2 = 64\n",
    "stride_conv1 = 1\n",
    "stride_conv2 = 1\n",
    "conv1_k = 5\n",
    "conv2_k = 5\n",
    "max_pool1_k = 2\n",
    "max_pool2_k = 2\n",
    "\n",
    "n_hidden = 1024\n",
    "n_out = 10\n",
    "\n",
    "input_size_to_hidden = (input_width//(max_pool1_k*max_pool2_k)) * (input_height//(max_pool1_k*max_pool2_k)) *n_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "MYNlFqwTmggp",
    "outputId": "8cef8e57-c1cc-44d7-f067-d2f48268b393",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialising the weights with random values\n",
    "# NOT USED IN THIS CODE WITH KERAS IMPLEMENTATION\n",
    "weights = {\n",
    "    \"wc1\" : tf.Variable(tf.random.normal([conv1_k, conv1_k, input_channels, n_conv1])),\n",
    "    \"wc2\" : tf.Variable(tf.random.normal([conv2_k, conv2_k, n_conv1, n_conv2])),\n",
    "    \"wh1\" : tf.Variable(tf.random.normal([input_size_to_hidden, n_hidden])),\n",
    "    \"wo\" : tf.Variable(tf.random.normal([n_hidden, n_out]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"bc1\" : tf.Variable(tf.random.normal([n_conv1])),\n",
    "    \"bc2\" : tf.Variable(tf.random.normal([n_conv2])),\n",
    "    \"bh1\" : tf.Variable(tf.random.normal([n_hidden])),\n",
    "    \"bo\" : tf.Variable(tf.random.normal([n_out])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psiAAeeymggr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#functions for layers\n",
    "def conv(x, weights, bias, strides = 1):\n",
    "    out = tf.nn.conv2d(x, weights, padding=\"SAME\", strides = [1, strides, strides, 1])\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    out = tf.nn.relu(out)\n",
    "    return out\n",
    "\n",
    "def maxpooling(x, k = 2):\n",
    "    return tf.nn.max_pool(x, padding = \"SAME\", ksize = [1, k, k, 1], strides = [1, k, k, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktMBnG1ymggt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function for forward prop\n",
    "#This function is edited now with keras functionality. weights are now handled internally but can still be accessed with get_weights method\n",
    "def cnn(x):\n",
    "    # Define the layers using Keras API\n",
    "    #x = layers.Reshape((input_height, input_width, input_channels))(x)\n",
    "    \n",
    "    conv1 = layers.Conv2D(n_conv1, kernel_size=(conv1_k, conv1_k), strides=stride_conv1, padding=\"same\", activation=\"relu\")(x)\n",
    "    conv1_pool = layers.MaxPooling2D(pool_size=(max_pool1_k, max_pool1_k))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(n_conv2, kernel_size=(conv2_k, conv2_k), strides=stride_conv2, padding=\"same\", activation=\"relu\")(conv1_pool)\n",
    "    conv2_pool = layers.MaxPooling2D(pool_size=(max_pool2_k, max_pool2_k))(conv2)\n",
    "\n",
    "    # Flatten the output from convolutional layers\n",
    "    flatten = layers.Flatten()(conv2_pool)\n",
    "\n",
    "    # Fully connected layer\n",
    "    hidden = layers.Dense(n_hidden, activation=\"relu\")(flatten)\n",
    "\n",
    "    # Dropout\n",
    "    dropout = layers.Dropout(0.2)(hidden)\n",
    "\n",
    "    # Output layer (logits)\n",
    "    output = layers.Dense(n_out, activation = 'softmax')(dropout)\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    #x = tf.reshape(x, shape = [-1 ,input_height, input_width, input_channels])\n",
    "    #conv1 = conv(x, weights['wc1'], biases['bc1'], stride_conv1)\n",
    "    #conv1_pool = maxpooling(conv1, max_pool1_k)\n",
    "    \n",
    "    #conv2 = conv(conv1_pool, weights['wc2'], biases['bc2'], stride_conv2)\n",
    "    #conv2_pool = maxpooling(conv2, max_pool2_k)\n",
    "    \n",
    "    #hidden_input = tf.reshape(conv2_pool, shape = [-1, input_size_to_hidden])\n",
    "    #hidden_output_before_activation = tf.add(tf.matmul(hidden_input, weights['wh1']), biases['bh1'])\n",
    "    #hidden_output_before_dropout = tf.nn.relu(hidden_output_before_activation)\n",
    "    #hidden_output = tf.nn.dropout(hidden_output_before_dropout, keep_prob) \n",
    "   \n",
    "    #output = tf.add(tf.matmul(hidden_output, weights['wo']), biases['bo']) #no softmax activation function since the loss function handles it already\n",
    "    #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Nd0sB1XOmggv",
    "outputId": "e2817129-39a4-4317-aaf7-0043d729a2ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x = tf.keras.Input(shape=(input_pixels,))\n",
    "x = tf.keras.Input(shape=(input_height, input_width, input_channels))\n",
    "y = tf.keras.Input(shape=(n_out,))\n",
    "#pred is the model\n",
    "pred = cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKNqv-KZmgg0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[x], outputs=pred)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #changed from 0.01 to 0.001\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "train_data = mnist_train.batch(32)\n",
    "test_data = mnist_test.batch(32)\n",
    "# Get a single batch from the test data (a batch is a tuple of (image, label))\n",
    "#single_batch = next(iter(test_data))\n",
    "\n",
    "# single_batch is a tuple, where single_batch[0] contains the image and single_batch[1] contains the label\n",
    "#image, label = single_batch\n",
    "\n",
    "# You can print or inspect the first image and label from the batch\n",
    "#print(image[0].numpy())  # The first image in the batch\n",
    "#print(label[0].numpy())  # The label for the first image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_samples(original, adversarial, index=None):\n",
    "    \"\"\"\n",
    "    Visualize original and adversarial samples side-by-side.\n",
    "    \n",
    "    Args:\n",
    "        original (np.array): The original sample (e.g., shape (28, 28, 1)).\n",
    "        adversarial (np.array): The adversarial sample (same shape as original).\n",
    "        index (int, optional): The index of the sample (useful for debugging).\n",
    "    \"\"\"\n",
    "    original = np.squeeze(original)  # Remove extra dimensions if present\n",
    "    adversarial = np.squeeze(adversarial)  # Remove extra dimensions if present\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    fig.suptitle(f\"Original vs. Adversarial (Index: {index})\", fontsize=16)\n",
    "    \n",
    "    # Plot original\n",
    "    axs[0].imshow(original, cmap='gray')\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Plot adversarial\n",
    "    axs[1].imshow(adversarial, cmap='gray')\n",
    "    axs[1].set_title(\"Adversarial\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partly_adv_data(batch_images, fraction=0.5):\n",
    "    batch_size = batch_images.shape[0]\n",
    "    num_adversarial = int(fraction * batch_size)\n",
    "    batch_images = np.array(batch_images)\n",
    "    original_batch_images = batch_images.copy()\n",
    "    \n",
    "    # Select random indices within the batch\n",
    "    indices = np.random.choice(batch_size, num_adversarial, replace=False)\n",
    "    \n",
    "    # Choose random ATN and generate adversarial samples\n",
    "    chosen_atn = np.random.choice(atns)\n",
    "    adversarial_examples = chosen_atn.predict(batch_images[indices], verbose=0)\n",
    "\n",
    "    # Create a new batch with a mix of original and adversarial examples\n",
    "    batch_images[indices] = adversarial_examples\n",
    "    \n",
    "    #for index in indices:\n",
    "    #    visualize_samples(original_batch_images[index], batch_images[index])\n",
    "    \n",
    "    # Convert back to TensorFlow tensor\n",
    "    batch_images = tf.convert_to_tensor(batch_images, dtype=tf.float32)\n",
    "    \n",
    "    return batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_train_step(batch_images, batch_labels):\n",
    "    mixed_images = create_partly_adv_data(batch_images, fraction=0.5)\n",
    "    loss, accuracy = model.train_on_batch(mixed_images, batch_labels)\n",
    "    return loss, accuracy  # Return both loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 14:54:49.410250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8400\n",
      "2025-01-29 14:54:57.009395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 938/938 - loss: 0.4732 - accuracy: 0.8750\n",
      "Epoch 1 completed in 149.19s - loss: 0.7014 - accuracy: 0.9142\n",
      "Epoch 2/160\n",
      "[==============================] 938/938 - loss: 0.9193 - accuracy: 0.8125\n",
      "Epoch 2 completed in 134.04s - loss: 0.1193 - accuracy: 0.9651\n",
      "Epoch 3/160\n",
      "[==============================] 938/938 - loss: 1.0247 - accuracy: 0.6875\n",
      "Epoch 3 completed in 137.30s - loss: 0.1031 - accuracy: 0.9685\n",
      "Epoch 4/160\n",
      "[==============================] 938/938 - loss: 0.5659 - accuracy: 0.8750\n",
      "Epoch 4 completed in 133.22s - loss: 0.0917 - accuracy: 0.9726\n",
      "Epoch 5/160\n",
      "[==============================] 938/938 - loss: 0.2739 - accuracy: 0.9375\n",
      "Epoch 5 completed in 133.11s - loss: 0.0827 - accuracy: 0.9754\n",
      "Epoch 6/160\n",
      "[==============================] 938/938 - loss: 0.3496 - accuracy: 0.9375\n",
      "Epoch 6 completed in 132.54s - loss: 0.0759 - accuracy: 0.9781\n",
      "Epoch 7/160\n",
      "[==============================] 938/938 - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 7 completed in 135.38s - loss: 0.0799 - accuracy: 0.9770\n",
      "Epoch 8/160\n",
      "[==============================] 938/938 - loss: 0.7159 - accuracy: 0.9375\n",
      "Epoch 8 completed in 136.67s - loss: 0.0749 - accuracy: 0.9787\n",
      "Epoch 9/160\n",
      "[==============================] 938/938 - loss: 0.1271 - accuracy: 0.9375\n",
      "Epoch 9 completed in 139.18s - loss: 0.0687 - accuracy: 0.9803\n",
      "Epoch 10/160\n",
      "[==============================] 938/938 - loss: 0.2527 - accuracy: 0.9375\n",
      "Epoch 10 completed in 137.87s - loss: 0.0642 - accuracy: 0.9825\n",
      "Epoch 11/160\n",
      "[==============================] 938/938 - loss: 0.1270 - accuracy: 0.9375\n",
      "Epoch 11 completed in 138.16s - loss: 0.0650 - accuracy: 0.9827\n",
      "Epoch 12/160\n",
      "[==============================] 938/938 - loss: 0.4191 - accuracy: 0.9375\n",
      "Epoch 12 completed in 140.99s - loss: 0.0606 - accuracy: 0.9835\n",
      "Epoch 13/160\n",
      "[==============================] 938/938 - loss: 0.1306 - accuracy: 0.8750\n",
      "Epoch 13 completed in 137.86s - loss: 0.0579 - accuracy: 0.9853\n",
      "Epoch 14/160\n",
      "[==============================] 938/938 - loss: 0.3680 - accuracy: 0.9375\n",
      "Epoch 14 completed in 140.79s - loss: 0.0639 - accuracy: 0.9845\n",
      "Epoch 15/160\n",
      "[==============================] 938/938 - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 15 completed in 137.58s - loss: 0.0515 - accuracy: 0.9867\n",
      "Epoch 16/160\n",
      "[==============================] 938/938 - loss: 0.0871 - accuracy: 0.9375\n",
      "Epoch 16 completed in 137.40s - loss: 0.0551 - accuracy: 0.9868\n",
      "Epoch 17/160\n",
      "[==============================] 938/938 - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 17 completed in 133.87s - loss: 0.0526 - accuracy: 0.9873\n",
      "Epoch 18/160\n",
      "[==============================] 938/938 - loss: 0.9182 - accuracy: 0.9375\n",
      "Epoch 18 completed in 134.70s - loss: 0.0561 - accuracy: 0.9862\n",
      "Epoch 19/160\n",
      "[==============================] 938/938 - loss: 0.0880 - accuracy: 0.9375\n",
      "Epoch 19 completed in 133.25s - loss: 0.0472 - accuracy: 0.9880\n",
      "Epoch 20/160\n",
      "[==============================] 938/938 - loss: 0.5115 - accuracy: 0.9375\n",
      "Epoch 20 completed in 138.26s - loss: 0.0579 - accuracy: 0.9870\n",
      "Epoch 21/160\n",
      "[==============================] 938/938 - loss: 0.3726 - accuracy: 0.9375\n",
      "Epoch 21 completed in 136.49s - loss: 0.0479 - accuracy: 0.9890\n",
      "Epoch 22/160\n",
      "[==============================] 938/938 - loss: 0.0469 - accuracy: 0.9375\n",
      "Epoch 22 completed in 134.30s - loss: 0.0610 - accuracy: 0.9869\n",
      "Epoch 23/160\n",
      "[==============================] 938/938 - loss: 0.1408 - accuracy: 0.9375\n",
      "Epoch 23 completed in 135.19s - loss: 0.0467 - accuracy: 0.9899\n",
      "Epoch 24/160\n",
      "[==============================] 938/938 - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 24 completed in 136.66s - loss: 0.0565 - accuracy: 0.9887\n",
      "Epoch 25/160\n",
      "[==============================] 938/938 - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 25 completed in 135.09s - loss: 0.0508 - accuracy: 0.9895\n",
      "Epoch 26/160\n",
      "[==============================] 938/938 - loss: 0.2331 - accuracy: 0.9375\n",
      "Epoch 26 completed in 135.68s - loss: 0.0497 - accuracy: 0.9903\n",
      "Epoch 27/160\n",
      "[==============================] 938/938 - loss: 0.0001 - accuracy: 1.0000\n",
      "Epoch 27 completed in 137.85s - loss: 0.0555 - accuracy: 0.9893\n",
      "Epoch 28/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 28 completed in 137.68s - loss: 0.0580 - accuracy: 0.9896\n",
      "Epoch 29/160\n",
      "[==============================] 938/938 - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 29 completed in 136.84s - loss: 0.0590 - accuracy: 0.9890\n",
      "Epoch 30/160\n",
      "[==============================] 938/938 - loss: 0.0005 - accuracy: 1.0000\n",
      "Epoch 30 completed in 137.51s - loss: 0.0444 - accuracy: 0.9905\n",
      "Epoch 31/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 31 completed in 133.86s - loss: 0.0514 - accuracy: 0.9915\n",
      "Epoch 32/160\n",
      "[==============================] 938/938 - loss: 0.1805 - accuracy: 0.9375\n",
      "Epoch 32 completed in 135.30s - loss: 0.0726 - accuracy: 0.9885\n",
      "Epoch 33/160\n",
      "[==============================] 938/938 - loss: 0.4730 - accuracy: 0.9375\n",
      "Epoch 33 completed in 136.27s - loss: 0.0556 - accuracy: 0.9913\n",
      "Epoch 34/160\n",
      "[==============================] 938/938 - loss: 0.6072 - accuracy: 0.9375\n",
      "Epoch 34 completed in 136.31s - loss: 0.0644 - accuracy: 0.9905\n",
      "Epoch 35/160\n",
      "[==============================] 938/938 - loss: 0.2759 - accuracy: 0.9375\n",
      "Epoch 35 completed in 135.00s - loss: 0.0534 - accuracy: 0.9913\n",
      "Epoch 36/160\n",
      "[==============================] 938/938 - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 36 completed in 136.93s - loss: 0.0413 - accuracy: 0.9923\n",
      "Epoch 37/160\n",
      "[==============================] 938/938 - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 37 completed in 136.75s - loss: 0.0589 - accuracy: 0.9904\n",
      "Epoch 38/160\n",
      "[==============================] 938/938 - loss: 0.8513 - accuracy: 0.9375\n",
      "Epoch 38 completed in 129.09s - loss: 0.0542 - accuracy: 0.9910\n",
      "Epoch 39/160\n",
      "[==============================] 938/938 - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 39 completed in 129.58s - loss: 0.0574 - accuracy: 0.9922\n",
      "Epoch 40/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 40 completed in 130.73s - loss: 0.0679 - accuracy: 0.9905\n",
      "Epoch 41/160\n",
      "[==============================] 938/938 - loss: 2.5662 - accuracy: 0.8750\n",
      "Epoch 41 completed in 131.66s - loss: 0.0576 - accuracy: 0.9919\n",
      "Epoch 42/160\n",
      "[==============================] 938/938 - loss: 0.0006 - accuracy: 1.0000\n",
      "Epoch 42 completed in 129.90s - loss: 0.0639 - accuracy: 0.9920\n",
      "Epoch 43/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 43 completed in 131.00s - loss: 0.0690 - accuracy: 0.9912\n",
      "Epoch 44/160\n",
      "[==============================] 938/938 - loss: 0.1246 - accuracy: 0.9375\n",
      "Epoch 44 completed in 129.09s - loss: 0.0617 - accuracy: 0.9923\n",
      "Epoch 45/160\n",
      "[==============================] 938/938 - loss: 0.0786 - accuracy: 0.9375\n",
      "Epoch 45 completed in 131.28s - loss: 0.0620 - accuracy: 0.9921\n",
      "Epoch 46/160\n",
      "[==============================] 938/938 - loss: 0.7769 - accuracy: 0.9375\n",
      "Epoch 46 completed in 130.33s - loss: 0.0719 - accuracy: 0.9917\n",
      "Epoch 47/160\n",
      "[==============================] 938/938 - loss: 0.0003 - accuracy: 1.0000\n",
      "Epoch 47 completed in 129.91s - loss: 0.0629 - accuracy: 0.9920\n",
      "Epoch 48/160\n",
      "[==============================] 938/938 - loss: 0.4713 - accuracy: 0.9375\n",
      "Epoch 48 completed in 132.03s - loss: 0.0682 - accuracy: 0.9916\n",
      "Epoch 49/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 49 completed in 131.34s - loss: 0.0504 - accuracy: 0.9938\n",
      "Epoch 50/160\n",
      "[==============================] 938/938 - loss: 0.0840 - accuracy: 0.9375\n",
      "Epoch 50 completed in 130.35s - loss: 0.0469 - accuracy: 0.9938\n",
      "Epoch 51/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 51 completed in 127.70s - loss: 0.0710 - accuracy: 0.9919\n",
      "Epoch 52/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 52 completed in 132.62s - loss: 0.0729 - accuracy: 0.9926\n",
      "Epoch 53/160\n",
      "[==============================] 938/938 - loss: 0.3156 - accuracy: 0.9375\n",
      "Epoch 53 completed in 130.13s - loss: 0.0556 - accuracy: 0.9933\n",
      "Epoch 54/160\n",
      "[==============================] 938/938 - loss: 0.1108 - accuracy: 0.9375\n",
      "Epoch 54 completed in 131.38s - loss: 0.0773 - accuracy: 0.9931\n",
      "Epoch 55/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 55 completed in 130.32s - loss: 0.0649 - accuracy: 0.9937\n",
      "Epoch 56/160\n",
      "[==============================] 938/938 - loss: 1.8295 - accuracy: 0.9375\n",
      "Epoch 56 completed in 131.11s - loss: 0.0587 - accuracy: 0.9945\n",
      "Epoch 57/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 57 completed in 131.68s - loss: 0.0418 - accuracy: 0.9951\n",
      "Epoch 58/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 58 completed in 128.96s - loss: 0.1123 - accuracy: 0.9908\n",
      "Epoch 59/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 59 completed in 129.90s - loss: 0.0538 - accuracy: 0.9947\n",
      "Epoch 60/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 60 completed in 131.72s - loss: 0.0510 - accuracy: 0.9950\n",
      "Epoch 61/160\n",
      "[==============================] 938/938 - loss: 0.0009 - accuracy: 1.0000\n",
      "Epoch 61 completed in 129.29s - loss: 0.0783 - accuracy: 0.9928\n",
      "Epoch 62/160\n",
      "[==============================] 938/938 - loss: 0.0817 - accuracy: 0.9375\n",
      "Epoch 62 completed in 130.23s - loss: 0.0532 - accuracy: 0.9942\n",
      "Epoch 63/160\n",
      "[==============================] 938/938 - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 63 completed in 129.86s - loss: 0.0588 - accuracy: 0.9945\n",
      "Epoch 64/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 64 completed in 129.79s - loss: 0.0733 - accuracy: 0.9928\n",
      "Epoch 65/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 65 completed in 128.29s - loss: 0.0728 - accuracy: 0.9942\n",
      "Epoch 66/160\n",
      "[==============================] 938/938 - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 66 completed in 130.74s - loss: 0.0783 - accuracy: 0.9941\n",
      "Epoch 67/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 67 completed in 129.73s - loss: 0.0640 - accuracy: 0.9939\n",
      "Epoch 68/160\n",
      "[==============================] 938/938 - loss: 0.0003 - accuracy: 1.0000\n",
      "Epoch 68 completed in 130.60s - loss: 0.0635 - accuracy: 0.9948\n",
      "Epoch 69/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 69 completed in 128.98s - loss: 0.0770 - accuracy: 0.9935\n",
      "Epoch 70/160\n",
      "[==============================] 938/938 - loss: 0.0005 - accuracy: 1.0000\n",
      "Epoch 70 completed in 127.97s - loss: 0.0694 - accuracy: 0.9944\n",
      "Epoch 71/160\n",
      "[==============================] 938/938 - loss: 0.0001 - accuracy: 1.0000\n",
      "Epoch 71 completed in 128.47s - loss: 0.1000 - accuracy: 0.9943\n",
      "Epoch 72/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 72 completed in 127.09s - loss: 0.0628 - accuracy: 0.9956\n",
      "Epoch 73/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 73 completed in 129.44s - loss: 0.0626 - accuracy: 0.9953\n",
      "Epoch 74/160\n",
      "[==============================] 938/938 - loss: 2.5014 - accuracy: 0.8750\n",
      "Epoch 74 completed in 131.33s - loss: 0.0829 - accuracy: 0.9952\n",
      "Epoch 75/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 75 completed in 129.79s - loss: 0.0826 - accuracy: 0.9949\n",
      "Epoch 76/160\n",
      "[==============================] 938/938 - loss: 7.0111 - accuracy: 0.9375\n",
      "Epoch 76 completed in 130.71s - loss: 0.0863 - accuracy: 0.9953\n",
      "Epoch 77/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 77 completed in 129.13s - loss: 0.0851 - accuracy: 0.9946\n",
      "Epoch 78/160\n",
      "[==============================] 938/938 - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 78 completed in 129.05s - loss: 0.0576 - accuracy: 0.9960\n",
      "Epoch 79/160\n",
      "[==============================] 938/938 - loss: 0.3924 - accuracy: 0.9375\n",
      "Epoch 79 completed in 127.82s - loss: 0.0812 - accuracy: 0.9947\n",
      "Epoch 80/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 80 completed in 129.69s - loss: 0.1032 - accuracy: 0.9941\n",
      "Epoch 81/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 81 completed in 129.40s - loss: 0.0863 - accuracy: 0.9951\n",
      "Epoch 82/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 82 completed in 129.78s - loss: 0.0581 - accuracy: 0.9966\n",
      "Epoch 83/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 83 completed in 130.03s - loss: 0.0734 - accuracy: 0.9956\n",
      "Epoch 84/160\n",
      "[==============================] 938/938 - loss: 5.1631 - accuracy: 0.9375\n",
      "Epoch 84 completed in 130.70s - loss: 0.1136 - accuracy: 0.9940\n",
      "Epoch 85/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 85 completed in 130.15s - loss: 0.0792 - accuracy: 0.9957\n",
      "Epoch 86/160\n",
      "[==============================] 938/938 - loss: 0.2981 - accuracy: 0.9375\n",
      "Epoch 86 completed in 130.12s - loss: 0.0583 - accuracy: 0.9965\n",
      "Epoch 87/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 87 completed in 131.19s - loss: 0.0820 - accuracy: 0.9953\n",
      "Epoch 88/160\n",
      "[==============================] 938/938 - loss: 0.3809 - accuracy: 0.9375\n",
      "Epoch 88 completed in 131.31s - loss: 0.0911 - accuracy: 0.9952\n",
      "Epoch 89/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 89 completed in 136.15s - loss: 0.1170 - accuracy: 0.9953\n",
      "Epoch 90/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 90 completed in 137.79s - loss: 0.0855 - accuracy: 0.9956\n",
      "Epoch 91/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 91 completed in 136.10s - loss: 0.0849 - accuracy: 0.9957\n",
      "Epoch 92/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 92 completed in 137.04s - loss: 0.0841 - accuracy: 0.9961\n",
      "Epoch 93/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 93 completed in 134.15s - loss: 0.1257 - accuracy: 0.9952\n",
      "Epoch 94/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 94 completed in 127.04s - loss: 0.1119 - accuracy: 0.9960\n",
      "Epoch 95/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 95 completed in 122.41s - loss: 0.0685 - accuracy: 0.9967\n",
      "Epoch 96/160\n",
      "[==============================] 938/938 - loss: 0.4468 - accuracy: 0.9375\n",
      "Epoch 96 completed in 123.81s - loss: 0.0984 - accuracy: 0.9965\n",
      "Epoch 97/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 97 completed in 123.31s - loss: 0.1057 - accuracy: 0.9956\n",
      "Epoch 98/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 98 completed in 122.90s - loss: 0.1120 - accuracy: 0.9958\n",
      "Epoch 99/160\n",
      "[==============================] 938/938 - loss: 2.1941 - accuracy: 0.9375\n",
      "Epoch 99 completed in 123.84s - loss: 0.0707 - accuracy: 0.9967\n",
      "Epoch 100/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 100 completed in 116.91s - loss: 0.0744 - accuracy: 0.9968\n",
      "Epoch 101/160\n",
      "[==============================] 938/938 - loss: 0.0000 - accuracy: 1.0000\n",
      "Epoch 101 completed in 109.59s - loss: 0.1214 - accuracy: 0.9955\n",
      "Epoch 102/160\n",
      "[==============================] 938/938 - loss: 3.0703 - accuracy: 0.9375\n",
      "Epoch 102 completed in 109.28s - loss: 0.0860 - accuracy: 0.9968\n",
      "Epoch 103/160\n",
      "[============..................] 383/938 - loss: 0.0000 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Get the next batch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_data_iterator)\n\u001b[0;32m---> 18\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Accumulate metrics\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mcnn_train_step\u001b[0;34m(batch_images, batch_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcnn_train_step\u001b[39m(batch_images, batch_labels):\n\u001b[0;32m----> 2\u001b[0m     mixed_images \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_partly_adv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_on_batch(mixed_images, batch_labels)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mcreate_partly_adv_data\u001b[0;34m(batch_images, fraction)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Choose random ATN and generate adversarial samples\u001b[39;00m\n\u001b[1;32m     11\u001b[0m chosen_atn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(atns)\n\u001b[0;32m---> 12\u001b[0m adversarial_examples \u001b[38;5;241m=\u001b[39m \u001b[43mchosen_atn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a new batch with a mix of original and adversarial examples\u001b[39;00m\n\u001b[1;32m     15\u001b[0m batch_images[indices] \u001b[38;5;241m=\u001b[39m adversarial_examples\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:2346\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2344\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2345\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2348\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1304\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1306\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:703\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 703\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:742\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    740\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 742\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from time import time\n",
    "\n",
    "num_epochs = 160  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_start_time = time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    train_data_iterator = iter(train_data)  # Create an iterator\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = len(train_data)  # Total number of batches\n",
    "\n",
    "    for batch_number in range(num_batches):  # Loop over batches\n",
    "        try:\n",
    "            # Get the next batch\n",
    "            x, labels = next(train_data_iterator)\n",
    "            loss, accuracy = cnn_train_step(x, labels)  \n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            # Progress bar display\n",
    "            progress = int((batch_number + 1) / num_batches * 30)  # 30-character progress bar\n",
    "            sys.stdout.write(\n",
    "                f\"\\r[{'=' * progress}{'.' * (30 - progress)}] \"\n",
    "                f\"{batch_number + 1}/{num_batches} - loss: {loss:.4f} - accuracy: {accuracy:.4f}\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        except StopIteration:\n",
    "            break  # In case the iterator runs out of data\n",
    "\n",
    "    # End of epoch metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    elapsed_time = time() - cnn_start_time\n",
    "    print(f\"\\nEpoch {epoch + 1} completed in {elapsed_time:.2f}s - loss: {avg_loss:.4f} - accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 1.0646 - accuracy: 0.9850\n",
      "Test accuracy: 0.9850000143051147\n",
      "summary: \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              3212288   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data) #dropout is automatically set to 1.0 when calling model.evaluate\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(\"summary: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('EAT_100e_001lr.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7. CNN-MNIST_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
