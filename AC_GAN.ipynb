{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Age5-ZGqCtLY"
      },
      "outputs": [],
      "source": [
        "#%pip install tensorflow tensorflow-datasets numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-u7YeIHADpz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras as K\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AiUKATSAx-l"
      },
      "source": [
        "### Saving and Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhNvZfWrAuMX"
      },
      "outputs": [],
      "source": [
        "def save(gan, generator, discriminator, model_folder, prefix=\"ACGAN\"):\n",
        "        \"\"\"\n",
        "        Save the model weights\n",
        "        Args:\n",
        "        - path: Path to save the model weights\n",
        "        - prefix: Prefix for the model weights\n",
        "        \"\"\"\n",
        "        if not os.path.exists(model_folder):\n",
        "            os.makedirs(model_folder)\n",
        "\n",
        "        K.models.save_model(generator, f\"{model_folder}/{prefix}/generator\")\n",
        "        K.models.save_model(discriminator, f\"{model_folder}/{prefix}/discriminator\")\n",
        "\n",
        "        # save model\n",
        "        K.models.save_model(gan, f\"{model_folder}/{prefix}/model\")\n",
        "\n",
        "def load(model_folder, prefix=\"ACGAN\"):\n",
        "    \"\"\"\n",
        "    Load the model weights\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    generator = K.models.load_model(f\"{model_folder}/{prefix}/generator\")\n",
        "    discriminator = K.models.load_model(f\"{model_folder}/{prefix}/discriminator\")\n",
        "\n",
        "    gan = K.models.load_model(f\"{model_folder}/{prefix}/model\")\n",
        "\n",
        "    gan.summary()\n",
        "    generator.summary()\n",
        "    discriminator.summary()\n",
        "\n",
        "    return generator, discriminator, gan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEpZGARzA1bs"
      },
      "source": [
        "# AC-GAN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qeHpDMA9TS"
      },
      "source": [
        "## AC-GAN Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to3PUI0oA5mY"
      },
      "outputs": [],
      "source": [
        "class Generator(K.Model):\n",
        "    \"\"\"\n",
        "    Generator component of AC-GAN for MNIST dataset\n",
        "\n",
        "    Args:\n",
        "    - latent_dim: Dimension of the latent space (generated as noise)\n",
        "    - n_classes: Number of classes(labels) in the dataset (default=10)\n",
        "\n",
        "    inherited from https://github.com/kochlisGit/Generative-Adversarial-Networks/blob/main/mnist-digits-acgan/digits-acgan.py\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, n_classes=10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Layers for Latent Inputs\n",
        "        self.dense1 = K.layers.Dense(units=7 * 7 * 256, use_bias=False)\n",
        "        self.bn1 = K.layers.BatchNormalization()\n",
        "        self.reshape1 = K.layers.Reshape(target_shape=[7, 7, 256])\n",
        "\n",
        "        # Layers for Label Inputs\n",
        "        self.embedding = K.layers.Embedding(input_dim=n_classes, output_dim=64)\n",
        "        self.dense2 = K.layers.Dense(units=7*7, use_bias=False)\n",
        "        self.bn2 = K.layers.BatchNormalization()\n",
        "        self.reshape2 = K.layers.Reshape(target_shape=(7, 7, 1))\n",
        "\n",
        "        # Layers for Merging Inputs (Combining Latent and Label Inputs)\n",
        "        self.conv1 = K.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding='same', use_bias=False)\n",
        "        self.bn3 = K.layers.BatchNormalization()\n",
        "        self.dropout1 = K.layers.Dropout(rate=0.4)\n",
        "        self.conv2 = K.layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
        "        self.bn4 = K.layers.BatchNormalization()\n",
        "        self.dropout2 = K.layers.Dropout(rate=0.4)\n",
        "        self.conv3 = K.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='tanh')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass of the generator\n",
        "        - latent_inputs: Random noise from the latent space, using for generating images\n",
        "        - label_inputs: Labels for the images to be generated\n",
        "        - training: Boolean flag for whether training or testing\n",
        "        \"\"\"\n",
        "        latent_inputs, label_inputs = inputs\n",
        "\n",
        "        # Latent Inputs Layer (Dense Layer + BatchNorm + ReLU + Reshape)\n",
        "        x1 = self.dense1(latent_inputs)\n",
        "        x1 = self.bn1(x1, training=training)\n",
        "        x1 = K.layers.LeakyReLU()(x1)\n",
        "        x1 = self.reshape1(x1)\n",
        "\n",
        "        # Process label inputs\n",
        "        x2 = self.embedding(label_inputs)\n",
        "        x2 = self.dense2(x2)\n",
        "        x2 = self.bn2(x2, training=training)\n",
        "        x2 = K.layers.LeakyReLU()(x2)\n",
        "        x2 = self.reshape2(x2)\n",
        "\n",
        "        #\n",
        "        merged_inputs = K.layers.Concatenate()([x1, x2])\n",
        "        x = self.conv1(merged_inputs)\n",
        "        x = self.bn3(x, training=training)\n",
        "        x = K.layers.LeakyReLU()(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn4(x, training=training)\n",
        "        x = K.layers.LeakyReLU()(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsR-WKBJBABU"
      },
      "source": [
        "## AC-GAN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3yrqKrnBCY2"
      },
      "outputs": [],
      "source": [
        "class Discriminator(K.Model):\n",
        "    \"\"\"\n",
        "    Discriminator component of AC-GAN for MNIST dataset\n",
        "\n",
        "    Args:\n",
        "    - n_classes: Number of classes(labels) in the dataset (default=10) which predicted (discriminated) by the Discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Define layers\n",
        "        self.gaussian_noise = K.layers.GaussianNoise(stddev=0.2)\n",
        "        self.conv1 = K.layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
        "        self.bn1 = K.layers.BatchNormalization()\n",
        "        self.dropout1 = K.layers.Dropout(rate=0.4)\n",
        "        self.conv2 = K.layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same', use_bias=False)\n",
        "        self.bn2 = K.layers.BatchNormalization()\n",
        "        self.dropout2 = K.layers.Dropout(rate=0.4)\n",
        "\n",
        "        # flatten layer\n",
        "        self.flatten = K.layers.Flatten()\n",
        "\n",
        "        # Output layers: 2 Dense Layer for validity and label prediction\n",
        "        self.dense1 = K.layers.Dense(units=1, activation='sigmoid') # dense layer for validity the image\n",
        "        self.dense2 = K.layers.Dense(units=n_classes, activation='softmax') # dense layer for classifying the label\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass of the discriminator\n",
        "        Args:\n",
        "        - inputs: Input images to be discriminated. Passing the input (generated by the Generator) through the Discriminator\n",
        "        and output the validity and label prediction\n",
        "        - training: Boolean flag for whether training or testing\n",
        "\n",
        "        Returns:\n",
        "        - validity: Validity of the input image that the discriminator predicts\n",
        "        - label: Label of the input image that the discriminator predicts\n",
        "        \"\"\"\n",
        "        x = self.gaussian_noise(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = K.layers.LeakyReLU()(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = K.layers.LeakyReLU()(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Output layers\n",
        "        validity = self.dense1(x)\n",
        "        label = self.dense2(x)\n",
        "\n",
        "        return validity, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHK84CyQBUxa"
      },
      "source": [
        "## AC-GAN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De4TvXlVBZAi"
      },
      "outputs": [],
      "source": [
        "class ACGAN(K.Model):\n",
        "    def __init__(self, generator, discriminator, latent_dim, n_classes=10):\n",
        "        super(ACGAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.generator_optimizer = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
        "        self.discriminator_optimizer = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "        # Define loss functions with label smoothing\n",
        "        self.binary_loss = K.losses.BinaryCrossentropy(label_smoothing=0.25) #\n",
        "        self.sparse_categorical_loss = K.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    def compile(self):\n",
        "        super(ACGAN, self).compile()\n",
        "\n",
        "        # Set the discriminator to not trainable initially\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Compile the combined model\n",
        "        self.compile(\n",
        "            optimizer=self.generator_optimizer,\n",
        "            loss=[self.binary_loss, self.sparse_categorical_loss]\n",
        "        )\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Training step for the ACGAN model\n",
        "        Args:\n",
        "        - data: A batch of real images getting from the dataset (i.e. MNIST), this contains the images and labels,\n",
        "        and the corresponding shape and size of the images\n",
        "        \"\"\"\n",
        "        x_batch, y_batch = data\n",
        "        batch_size = tf.shape(x_batch)[0]\n",
        "\n",
        "        # =========================== Ground Truth labels =======================================\n",
        "        real_labels = tf.ones((batch_size, 1))\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "        mixed_labels = tf.concat([real_labels, fake_labels], axis=0)\n",
        "        #========================================================================================\n",
        "\n",
        "\n",
        "\n",
        "        # ====================== Generate the Noise for Discriminator ===========================\n",
        "\n",
        "        # Generate random noise and random labels from the latent space\n",
        "        random_latent_noise = tf.random.normal(shape=[batch_size, self.latent_dim])\n",
        "        random_labels = tf.random.uniform(shape=[batch_size], minval=0, maxval=self.n_classes, dtype=tf.int32) # Categorical labels\n",
        "\n",
        "        # Generate images from random noise and labels by Generator\n",
        "        generated_images = self.generator([random_latent_noise, random_labels], training=True)\n",
        "\n",
        "        # Mixed the real and generated images and labels for Discriminator (Concatenating)\n",
        "        mixed_images = tf.concat([x_batch, generated_images], axis=0)\n",
        "        mixed_generated_labels = tf.concat([y_batch, random_labels], axis=0)\n",
        "\n",
        "        #========================================================================================\n",
        "\n",
        "\n",
        "        # =========================== Train the Discriminator ====================================\n",
        "        self.discriminator.trainable = True # Set the discriminator to trainable\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            discriminated_validity, discriminated_label = self.discriminator(mixed_images, training=True)\n",
        "\n",
        "            discriminator_loss = [\n",
        "                self.binary_loss(mixed_labels, discriminated_validity), # validity loss\n",
        "                self.sparse_categorical_loss(mixed_generated_labels, discriminated_label) #label loss\n",
        "            ]\n",
        "\n",
        "            total_discriminator_loss = tf.reduce_mean(discriminator_loss[0]) + tf.reduce_mean(discriminator_loss[1])\n",
        "\n",
        "        gradients_D = tape.gradient(total_discriminator_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.discriminator_optimizer.apply_gradients(zip(gradients_D, self.discriminator.trainable_variables))\n",
        "\n",
        "        #========================================================================================\n",
        "\n",
        "\n",
        "\n",
        "        # =========================== Train the Generator =================================================\n",
        "        self.discriminator.trainable = False # Set the discriminator to not trainable\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self.generator([random_latent_noise, random_labels], training=True)\n",
        "            discriminated_validity, discriminated_label = self.discriminator(generated_images, training=False)\n",
        "\n",
        "            generator_loss = [\n",
        "                self.binary_loss(real_labels, discriminated_validity),\n",
        "                self.sparse_categorical_loss(random_labels, discriminated_label)\n",
        "            ]\n",
        "\n",
        "            total_generator_loss = tf.reduce_mean(generator_loss[0]) + tf.reduce_mean(generator_loss[1])\n",
        "\n",
        "        gradients_G = tape.gradient(total_generator_loss, self.generator.trainable_variables)\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_G, self.generator.trainable_variables))\n",
        "\n",
        "        #========================================================================================\n",
        "\n",
        "        return {\n",
        "            \"d_loss\": total_discriminator_loss,\n",
        "            \"g_loss\": total_generator_loss\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_images(self, latent_space, labels):\n",
        "        \"\"\"\n",
        "        Generate images from the latent space and labels. Using Generator only.\n",
        "        Args:\n",
        "        - latent_space: Random noise from the latent space\n",
        "        - labels: Labels for the images to be generated\n",
        "        \"\"\"\n",
        "        return self.generator([latent_space, labels], training=False)\n",
        "\n",
        "\n",
        "# ================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ukblvkBm1H"
      },
      "source": [
        "## Training AC-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiOLn-YKCNNU"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-sHChHcCRRH"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = K.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize to [-1, 1] and add channel dimension\n",
        "x_train = x_train.astype(np.float32) / 127.5 - 1\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\n",
        "x_test = x_test.astype(np.float32) / 127.5 - 1\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = K.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = K.utils.to_categorical(y_test, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQmcx5XSCWX-"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExZoBeZoCZuz"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "latent_dim = 128\n",
        "n_classes = 10\n",
        "batch_size = 64\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx7ZEUDcCb35"
      },
      "source": [
        "### Create Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DtGmFE_CDdC"
      },
      "outputs": [],
      "source": [
        "# Create the AC-GAN model\n",
        "generator = Generator(latent_dim, n_classes)\n",
        "discriminator = Discriminator(n_classes)\n",
        "\n",
        "acgan = ACGAN(generator, discriminator, latent_dim, n_classes)\n",
        "\n",
        "acgan.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYGDT_GwCkGL"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAk5c_F2CmG3"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "# # Create the dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(buffer_size=x_train.shape[0])\n",
        "inputs = train_dataset.batch(batch_size=batch_size, drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "batches_per_epoch = x_train.shape[0] // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for i, (x_batch, y_batch) in enumerate(inputs):\n",
        "        losses = acgan.train_step([x_batch, y_batch])\n",
        "\n",
        "        print(f\"\\rBatch {i+1}/{batches_per_epoch} \\n - Discriminator Loss: {losses['d_loss']:.4f} - Generator Loss: {losses['g_loss']:.4f}\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFfoO-2hDNbh"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p0XmVnMDRte"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_folder = \"models\"\n",
        "save(acgan, generator, discriminator, model_folder, prefix=\"ACGAN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BDJneLRBiqD"
      },
      "source": [
        "# AT - GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGsKp9cyBt99"
      },
      "source": [
        "## Target Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NceTp0bIBmUF"
      },
      "outputs": [],
      "source": [
        "# AT-GAN MODELS: Extended from ACGAN for Adversarial Attack\n",
        "class TargetClassifier(K.Model):\n",
        "    \"\"\"\n",
        "    Target Classifier for the AT-GAN model.\n",
        "    This simply acts as the classifier for the input images (MNIST) of either real or generated images.\n",
        "    Using as the target for the attack.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(TargetClassifier, self).__init__()\n",
        "\n",
        "        # Classifier Layers\n",
        "        self.conv1 = K.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1))\n",
        "        self.pool1 = K.layers.MaxPooling2D((2, 2))\n",
        "        self.conv2 = K.layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n",
        "        self.pool2 = K.layers.MaxPooling2D((2, 2))\n",
        "\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwbPAPUdBr9I"
      },
      "source": [
        "## Attack Generator (`G_attack`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZP8IkqJB2aS"
      },
      "outputs": [],
      "source": [
        "class Attack_Generator(K.Model):\n",
        "    \"\"\"\n",
        "    G_attack simply a copy of AC-GAN Generator, and used for the adversarial attack.\n",
        "    Which transfering the output of the Generator to the Target Classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, generator):\n",
        "        super(Attack_Generator, self).__init__()\n",
        "        self.generator = generator\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.generator(inputs, training=training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NxWKxJyB5GA"
      },
      "source": [
        "## AT-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJp7VSTxB7iD"
      },
      "outputs": [],
      "source": [
        "class ATGAN:\n",
        "    def __init__(self, G_original, G_attack, f_target, noise_size, lambda_adv_at=2.0, lambda_dist=1.0):\n",
        "        self.G_original = G_original # Original Generator (G_original)\n",
        "        self.G_attack = G_attack # Adversarial Generator (G_attack)\n",
        "        self.f_target = f_target    # Target Classifier (f_target)\n",
        "\n",
        "        self.noise_size = noise_size # latent space size\n",
        "\n",
        "        self.lambda_adv_at = lambda_adv_at  # lambda for adversarial loss\n",
        "        self.lambda_dist = lambda_dist     # lambda for distance loss\n",
        "\n",
        "        self.optimizer_G_attack = K.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.sparse_categorical_loss = K.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_atgan(self, images, target_labels):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "\n",
        "        with tf.GradientTape() as g_attack_tape:\n",
        "            z = tf.random.normal([batch_size, self.noise_size])\n",
        "\n",
        "            # Generate adversarial images\n",
        "            adv_images = self.G_attack([z, target_labels], training=True)\n",
        "\n",
        "            # Target classifier's prediction on adversarial images\n",
        "            pred_adv = self.f_target(adv_images, training=False)\n",
        "\n",
        "            # 1. Adversarial Loss (La) ========================================================\n",
        "\n",
        "            la_loss = tf.reduce_mean(\n",
        "                self.sparse_categorical_loss(target_labels, pred_adv)\n",
        "            )\n",
        "\n",
        "            # 2. Distance Loss (Ld) ========================================================\n",
        "            # Add Gaussian noise\n",
        "            noise = tf.random.normal(shape=tf.shape(adv_images), mean=0.0, stddev=0.1)\n",
        "            adv_images_noisy = adv_images + noise\n",
        "\n",
        "            # Original images generated by G_original\n",
        "            orig_images = self.G_original([z, target_labels], training=False)\n",
        "\n",
        "            ld_loss = tf.reduce_mean(tf.square(orig_images - adv_images_noisy))\n",
        "\n",
        "            # Total adversarial loss for G_attack\n",
        "            g_attack_loss = self.lambda_adv_at * la_loss + self.lambda_dist * ld_loss\n",
        "\n",
        "        # Calculate G_attack gradients\n",
        "        g_attack_gradients = g_attack_tape.gradient(g_attack_loss, self.G_attack.trainable_variables)\n",
        "        self.optimizer_G_attack.apply_gradients(zip(g_attack_gradients, self.G_attack.trainable_variables))\n",
        "\n",
        "        return g_attack_loss, la_loss, ld_loss\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B7qeHpDMA9TS",
        "VGsKp9cyBt99",
        "lwbPAPUdBr9I",
        "3NxWKxJyB5GA"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
